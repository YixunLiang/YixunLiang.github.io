<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yixun Liang</title>
  
  <meta name="author" content="Yixun Liang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yixun Liang (Ê¢ÅÈÄ∏Âãã)</name>
              </p>
              <p>I am a Ph.D student at <a href="https://hkust.edu.hk/">HKUST</a> starting in Jan 2025, supervised by Prof. <a href="https://pingtan.people.ust.hk/index.html">Ping TAN</a>. Previously, I obtained my M.phil degree in Artificial Intelligence at <a href="https://hkust-gz.edu.cn/">HKUST(GZ)</a>, supervised by Prof. <a href="https://www.yingcong.me/">Ying-Cong CHEN</a>, and Bachelor degree in Information Engneering at <a href="https://www.uestc.edu.cn/">UESTC</a> with Honor Research, cooperate with Prof.<a href="https://yjsjy.uestc.edu.cn/gmis/jcsjgl/dsfc/dsgrjj/11316?yxsh=05">Ping Zhang</a>.
              </p>
              <p>
                I am a member of <a target=_blank href="https://anysyn3d.github.io/">AnySyn3D</a>, a non-profit research interest group comprising individuals with a strong interest in exploring research problems and cutting-edge technologies in 3D AIGC. If you are interested in discussing with me, feel free to drop me an email.
              </p>
              <p style="text-align:center">
                <a href="mailto:lyxun2000@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/YixunLiang">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=rmX3lvEAAAAJ">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/YixunLiang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/YixunLiang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Research Interest</heading>
              <p>
                My current research interests mainly focus on the field of 3D understanding, reconstruction and generation. 
              </p>
            </td>
          </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>
                Full Publications can be found in <a href="https://scholar.google.com/citations?user=rmX3lvEAAAAJ">Google Scholar</a>.
              </p>
            </td>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()"></tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2505.23253">
                <papertitle>UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes</papertitle>
              </a>
              <br>
              <strong>Yixun Liang*,</strong>
              Kunming Luo*,
              Xiao Chen*,
              Rui Chen,
              Hongyu Yan,
              Weiyu Li,
              Jiarui Liu,
              Ping Tan
              <br>
              <em>CVPR</em>, 2026
              <br>
              <a href="https://arxiv.org/abs/2505.23253">Paper</a> /
              <a href="https://github.com/lightillusions/UniTEX">Code</a> /
              <a href="https://www.youtube.com/watch?v=O8G1XqfIxck&feature=youtu.be">Video</a>
              <br>
              <em>TL;DR: A 3D texture system that combined fine-tuned large-scale Diffusion Transformers (DiT) with an advanced LoRA-based training strategy and native 3D texturing pipeline (Large Texturing Model, LTM), bypassing UV mapping limitations to deliver high-fidelity, consistent textures for 3D shapes. </em>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()"></tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/10.1145/3759249">
                <papertitle>Iris3D: 3D Generation via Synchronized Diffusion Distillation</papertitle>
              </a>
              <br>
              <strong>Yixun Liang</strong>,
              Weiyu Li,
              Rui Chen,
              Fei-Peng Tian,
              Jiarui Liu,
              Ying-Cong Chen,
              Ping Tan,
              Xiao-Xiao Long,
              <br>
              <em>ACM Transactions on Graphics</em>, 2025 &nbsp <font color="red"><strong>(Present at SIGGRAPH 2026)</strong></font>
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3759249">Paper</a>
              <br>
              <em>TL;DR: Building upon LucidDreamer, we address the multi-view inconsistency under the distillation paradigm, achieve training-free multi-view consistent SDS, and integrate it with state-of-the-art feed-forward models to form a brand-new 3D generation system.</em>
              <p></p>            
            </td>
          </tr>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()"></tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.11284">
                <papertitle>LucidDreamer: Towards High-Fidelity Text-to-3D Generation
                  via Interval Score Matching</papertitle>
              </a>
              <br>
              <strong>Yixun Liang*</strong>,
              Xin Yang*,
              Jiantao Lin,
              Haodong Li,
              Xiaogang Xu,
              Ying-Cong Chen
              <br>
              <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Highlight)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2311.11284">Paper</a> /
              <a href="https://github.com/EnVision-Research/LucidDreamer">Code</a>
              <br>
              <em>TL;DR: Discovering that the intrinsic mechanism of Score Distillation Sampling (SDS) involves predicting pseudo targets based on conditions, then introduce DDIM inversion to reduce the averaging effect, alongside proposing Interval Score Matching (ISM) and integrating 3D Gaussian Splatting for high-fidelity text-to-3D generation.</em>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="retr_stop()" onmouseover="retr_start()">
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.18832">
                <papertitle>ReTR: Modeling Rendering via Transformer for Generalizable Neural Surface Reconstruction</papertitle>
              </a>
              <br>
              <strong>Yixun Liang*</strong>,
              Hao He*,
              Ying-Cong Chen
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2305.18832">Paper</a> /
              <a href="https://YixunLiang.github.io/ReTR/">Project Page</a> /
              <a href="https://github.com/YixunLiang/ReTR">Code</a>
              <br>
              <em>TL;DR: Discover that attention mechanisms tend to form a unimodal property of attention scores during training, and apply this feature to rendering to improve the accuracy of depth estimation.</em>
              <p></p>
            </td>
          </tr>


          <tr onmouseout="mantra_stop()" onmouseover="mantra_start()">
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.10585">
                <papertitle>Label Name is Mantra: Unifying Point Cloud Segmentation across Heterogeneous Datasets</papertitle>
              </a>
              <br>
              <strong>Yixun Liang*</strong>,
              Hao He*,
              Shi-shi Xiao,
              Hao Lu,
              Ying-Cong Chen
              <!-- (* denotes equal contribution)-->
              <br>
              <em>Pacific Graphics</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2303.10585">Paper</a> /
              <br>
              <em>TL;DR: Utilizing a pre-trained language model to embed point cloud segmentation labels into a unified continuous latent space, our approach enables the joint training of heterogeneous datasets with label mismatches..</em>
              <p></p>
            </td>
          </tr>		
         
          <tr onmouseout="mantra_stop()" onmouseover="mantra_start()">
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2405.14979">
                <papertitle>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner</papertitle>
              </a>
              <br>
              Weiyu Li*, Jiarui Liu*, Hongyu Yan*, Rui Chen, <strong>Yixun Liang</strong>, Xuelin Chen, Ping Tan, Xiaoxiao Long
              <!-- (* denotes equal contribution)-->
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2405.14979">Paper</a> /
              <a href="https://craftsman3d.github.io/">Project Page</a> /
              <a href="https://github.com/HKUST-SAIL/CraftsMan3D">Code</a>
              <br>
              <em>TL;DR: A novel 2-stage native 3D diffusion for 3D object generation.</em>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="mantra_stop()" onmouseover="mantra_start()">
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2412.17808">
                <papertitle>Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders</papertitle>
              </a>
              <br>
              Rui Chen, Jianfeng Zhang*, <strong>Yixun Liang</strong>, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, Ping Tan,
              <!-- (* denotes equal contribution)-->
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2412.17808">Paper</a> /
              <a href="https://aruichen.github.io/Dora/">Project Page</a> /
              <a href="https://github.com/Seed3D/Dora">Code</a>
              <br>
              <em>TL;DR: sharp edge sampling for 3D VAE.</em>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="mantra_stop()" onmouseover="mantra_start()">
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2409.18124">
                <papertitle>Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction</papertitle>
              </a>
              <br>
              Jing He* , Haodong Li* , Wei Yin, <strong>Yixun Liang</strong>, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, Ying-Cong Chen
              <!-- (* denotes equal contribution)-->
              <br>
              <em>ICLR</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2409.18124">Paper</a> /
              <a href="https://arxiv.org/abs/2409.18124">Project Page</a> /
              <a href="https://github.com/EnVision-Research/Lotus">Code</a>
              <br>
              <em>TL;DR: A diffusion based depth prediction system.</em>
              <p></p>
            </td>
          </tr>
          
          <tr onmouseout="mantra_stop()" onmouseover="mantra_start()">
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_Baking_Gaussian_Splatting_into_Diffusion_Denoiser_for_Fast_and_Scalable_ICCV_2025_paper.pdf">
                <papertitle>Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation and Reconstruction</papertitle>
              </a>
              <br>
              Yuanhao Cai, He Zhang, Kai Zhang, <strong>Yixun Liang‚Ä†</strong>, Mengwei Ren, Fujun Luan, Qing Liu, Soo Ye Kim, Jianming Zhang, Zhifei Zhang, Yuqian Zhou, Yulun Zhang‚Ä†, Xiaokang Yang, Zhe Lin, Alan Yuille
              <!-- (* denotes equal contribution)-->
              <br>
              <em>ICCV</em>, 2025
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Cai_Baking_Gaussian_Splatting_into_Diffusion_Denoiser_for_Fast_and_Scalable_ICCV_2025_paper.pdf">Paper</a> /
              <a href="https://caiyuanhao1998.github.io/project/DiffusionGS/">Project Page</a> /
              <a href="https://github.com/caiyuanhao1998/Open-DiffusionGS">Code</a>
              <br>
              <em>TL;DR: A novel one-stage 3DGS-based diffusion for 3D object generation and scene reconstruction from a single view in ~6 seconds.</em>
              <p></p>
            </td>
          </tr>	
        </tbody></table>

				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                thanks for the <a href="https://github.com/jonbarron/jonbarron_website">awesome template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
