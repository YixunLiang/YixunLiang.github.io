<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fangjinhua Wang</title>
  
  <meta name="author" content="Fangjinhua Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fangjinhua Wang (ÁéãÊñπÈî¶Âçé)</name>
              </p>
              <p>I am a Ph.D. student in Computer Science at <a href="https://ethz.ch/en.html">ETH Zurich</a>, supervised by Prof. <a href="http://cvg.ethz.ch/people/faculty/">Marc Pollefeys</a>. 
              </p>
              <p>
                Previously, I obtained a Master‚Äôs degree in Robotics at ETH zurich with distinction. I am the recipient of Chu Kochen Award (highest honor at Zhejiang University, 2017) and <a href="https://ethz.ch/content/dam/ethz/main/eth-zurich/education/auszeichnungen-preise/files/eth-medaille/medaille-master-arbeit-2021.pdf">ETH Medal</a> (ETH Zurich, 2021). 
              </p>
              <p style="text-align:center">
                <a href="mailto:fangjinhua.wang@inf.ethz.ch">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ysTmrEsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/FangjinhuaWang">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/fangjinhua-wang-4ba2aa150/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/FangjinhuaWang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/FangjinhuaWang_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My current research interests mainly lie in the field of 3D vision, dense matching, reconstruction and deep learning. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
		      <tr onmouseout="volrecon_stop()" onmouseover="volrecon_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='volrecon_image'>
                  <img src='images/volrecon_after.jpg' width="160"></div>
                <img src='images/volrecon_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function volrecon_start() {
                  document.getElementById('volrecon_image').style.opacity = "1";
                }

                function volrecon_stop() {
                  document.getElementById('volrecon_image').style.opacity = "0";
                }
                volrecon_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2212.08067">
                <papertitle>VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction</papertitle>
              </a>
              <br>
              Yufan Ren*,
              <strong>Fangjinhua Wang*</strong>,
              Tong Zhang,
              Marc Pollefeys,
              Sabine S√ºsstrunk
              (* denotes equal contribution)
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2212.08067">Paper</a> /
              <a href="https://fangjinhuawang.github.io/VolRecon/">Project Page</a> /
              <a href="https://github.com/IVRL/VolRecon">Code</a>
              <p></p>
            </td>
          </tr>   

          <tr onmouseout="itermvs_stop()" onmouseover="itermvs_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='itermvs_image'>
                  <img src='images/itermvs_after.jpg' width="160"></div>
                <img src='images/itermvs_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function itermvs_start() {
                  document.getElementById('itermvs_image').style.opacity = "1";
                }

                function itermvs_stop() {
                  document.getElementById('itermvs_image').style.opacity = "0";
                }
                itermvs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_IterMVS_Iterative_Probability_Estimation_for_Efficient_Multi-View_Stereo_CVPR_2022_paper.pdf">
                <papertitle>IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo</papertitle>
              </a>
              <br>
              <strong>Fangjinhua Wang</strong>,
              Silvano Galliani, 
              Christoph Vogel, 
              Marc Pollefeys
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_IterMVS_Iterative_Probability_Estimation_for_Efficient_Multi-View_Stereo_CVPR_2022_paper.pdf">Paper</a> /
              <a href="https://github.com/FangjinhuaWang/IterMVS">Code</a>
              <p></p>
            </td>
          </tr>		
          

          <tr onmouseout="patchmatchnet_stop()" onmouseover="patchmatchnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='patchmatchnet_image'>
                  <img src='images/patchmatchnet_after.jpg' width="160"></div>
                <img src='images/patchmatchnet_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function patchmatchnet_start() {
                  document.getElementById('patchmatchnet_image').style.opacity = "1";
                }

                function patchmatchnet_stop() {
                  document.getElementById('patchmatchnet_image').style.opacity = "0";
                }
                patchmatchnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PatchmatchNet_Learned_Multi-View_Patchmatch_Stereo_CVPR_2021_paper.pdf">
                <papertitle>PatchmatchNet: Learned Multi-View Patchmatch Stereo</papertitle>
              </a>
              <br>
              <strong>Fangjinhua Wang</strong>,
              Silvano Galliani, 
              Christoph Vogel, 
              Pablo Speciale, 
              Marc Pollefeys
              <br>
              <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PatchmatchNet_Learned_Multi-View_Patchmatch_Stereo_CVPR_2021_paper.pdf">Paper</a> /
              <a href="https://github.com/FangjinhuaWang/PatchmatchNet">Code</a>
              <p></p>
            </td>
          </tr>   


          <tr onmouseout="multisensorwearable_stop()" onmouseover="multisensorwearable_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='multisensorwearable_image'>
                  <img src='images/multisensorwearable_after.jpg' width="160"></div>
                <img src='images/multisensorwearable_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function multisensorwearable_start() {
                  document.getElementById('multisensorwearable_image').style.opacity = "1";
                }

                function multisensorwearable_stop() {
                  document.getElementById('multisensorwearable_image').style.opacity = "0";
                }
                multisensorwearable_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9628050/">
                <papertitle>Self-Calibrated Multi-Sensor Wearable for Hand Tracking and Modeling</papertitle>
              </a>
              <br>
              Nikhil Bharadwaj Gosala*,
              <strong>Fangjinhua Wang*</strong>,
              Zhaopeng Cui, 
              Hanxue Liang, 
              Oliver Glauser, 
              Shihao Wu, 
              Olga Sorkine-Hornung
              (* denotes equal contribution)
              <br>
              <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2021
              <p></p>
            </td>
          </tr>   
         

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td width="75%" valign="center">
              Meta Reality Labs (Redmond, US), 2022
              <br>
              Microsoft Mixed Reality & AI Zurich Lab (Zurich, Switzerland), 2020
            </td>
          </tr>
          
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td width="75%" valign="center">
              <strong>Journal reviewer</strong>: TVCG, CAG, Neural Computing, TCSVT, TIP
              <br>
              <strong>Conference reviewer</strong>: ECCV2022, ICRA2023, CVPR2023
            </td>
          </tr>
          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                thanks for the <a href="https://github.com/jonbarron/jonbarron_website">awesome template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
